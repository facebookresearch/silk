<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>silk.data.loader API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>silk.data.loader</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.

# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

from collections.abc import Iterator
from typing import List, Union, Dict, Any

import torch.utils.data
from torch.utils.data.dataset import Dataset, IterableDataset


class StatefulDataLoader(torch.utils.data.DataLoader):
    &#34;&#34;&#34;Stateful version of a PyTorch DataLoader. Stateful in a sense that subsequent calls to `__iter__` will continue previously created iterator.
    The returned iterator is essentially shared.

    This is especially useful when dealing with infinite or very large datasets. In those cases, we have to limit the epoch size during training.
    However, that introduces a problem : Most training cycle implementation will reset the dataloader iterator by calling `iter(dataloader)`, which makes
    us train on the same initial data over and over again. A `StatefulDataLoader` solves that issue by making calls to `__iter__` return the same shared iterator.

    Examples
    --------

    ```python
    from torch.utils.data import IterableDataset
    from silk.data.loader import StatefulDataLoader
    import itertools

    class MyDataset(IterableDataset):
        def __iter__(self):
            n = 1
            while True:
                yield n
                n += 1

    dataset = MyDataset()
    dataloader = StatefulDataLoader(dataset, batch_size=None, collate_fn=lambda x: x)

    print(list(itertools.islice(dataloader, 5)))
    # &gt;&gt;&gt; [1, 2, 3, 4, 5]
    print(list(itertools.islice(dataloader, 5)))
    # &gt;&gt;&gt; [6, 7, 8, 9, 10]
    ```
    &#34;&#34;&#34;

    class _Iterator(Iterator):
        &#34;&#34;&#34;Iternal iterator.

        IMPORTANT : This class HAS to subclass the abstract class `Iterator` since PyTorch Lightning runs the `next()` function on iterators by filtering them by class.
        In case this super class is removed, the iterator will not be processed properly.&#34;&#34;&#34;

        def __init__(self, data_loader, cycle) -&gt; None:
            super().__init__()
            self._cycle = cycle
            self._dataloader = data_loader
            self._reset_iterator()

        def _reset_iterator(self):
            self._iterator = torch.utils.data.DataLoader.__iter__(self._dataloader)

        def __next__(self):
            if self._cycle:
                try:
                    return next(self._iterator)
                except StopIteration:
                    self._reset_iterator()
            return next(self._iterator)

    def __init__(
        self,
        dataset: Union[Dataset, IterableDataset],
        *args: List[Any],
        cycle: bool = False,
        **kwargs: Dict[str, Any],
    ):
        &#34;&#34;&#34;

        Parameters
        ----------
        dataset : Union[Dataset, IterableDataset]
            Dataset provided to the dataloader (c.f. torch.utils.data.dataset.DataLoader).
        *args : List[Any]
            Positional arguments passed to the `DataLoader`.
        cycle : bool, optional
            Automatically reset the iterator once the end of the dataset is reached, by default False.
        **kwargs : Dict[str, Any]
            Keyword arguments passed to the `DataLoader`.
        &#34;&#34;&#34;
        super().__init__(dataset, *args, **kwargs)
        self._cycle = cycle
        self._stateful_iterator = None

    def reset(self):
        &#34;&#34;&#34;Resets the shared iterator to the beginning of the dataset.&#34;&#34;&#34;
        self._stateful_iterator = StatefulDataLoader._Iterator(self, self._cycle)

    def __iter__(self):
        if self._stateful_iterator is None:
            self.reset()
        return self._stateful_iterator</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="silk.data.loader.StatefulDataLoader"><code class="flex name class">
<span>class <span class="ident">StatefulDataLoader</span></span>
<span>(</span><span>dataset: Union[torch.utils.data.dataset.Dataset, torch.utils.data.dataset.IterableDataset], *args: List[Any], cycle: bool = False, **kwargs: Dict[str, Any])</span>
</code></dt>
<dd>
<div class="desc"><p>Stateful version of a PyTorch DataLoader. Stateful in a sense that subsequent calls to <code>__iter__</code> will continue previously created iterator.
The returned iterator is essentially shared.</p>
<p>This is especially useful when dealing with infinite or very large datasets. In those cases, we have to limit the epoch size during training.
However, that introduces a problem : Most training cycle implementation will reset the dataloader iterator by calling <code>iter(dataloader)</code>, which makes
us train on the same initial data over and over again. A <code><a title="silk.data.loader.StatefulDataLoader" href="#silk.data.loader.StatefulDataLoader">StatefulDataLoader</a></code> solves that issue by making calls to <code>__iter__</code> return the same shared iterator.</p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python">from torch.utils.data import IterableDataset
from silk.data.loader import StatefulDataLoader
import itertools

class MyDataset(IterableDataset):
    def __iter__(self):
        n = 1
        while True:
            yield n
            n += 1

dataset = MyDataset()
dataloader = StatefulDataLoader(dataset, batch_size=None, collate_fn=lambda x: x)

print(list(itertools.islice(dataloader, 5)))
# &gt;&gt;&gt; [1, 2, 3, 4, 5]
print(list(itertools.islice(dataloader, 5)))
# &gt;&gt;&gt; [6, 7, 8, 9, 10]
</code></pre>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>Union[Dataset, IterableDataset]</code></dt>
<dd>Dataset provided to the dataloader (c.f. torch.utils.data.dataset.DataLoader).</dd>
<dt><strong><code>*args</code></strong> :&ensp;<code>List[Any]</code></dt>
<dd>Positional arguments passed to the <code>DataLoader</code>.</dd>
<dt><strong><code>cycle</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Automatically reset the iterator once the end of the dataset is reached, by default False.</dd>
<dt><strong><code>**kwargs</code></strong> :&ensp;<code>Dict[str, Any]</code></dt>
<dd>Keyword arguments passed to the <code>DataLoader</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class StatefulDataLoader(torch.utils.data.DataLoader):
    &#34;&#34;&#34;Stateful version of a PyTorch DataLoader. Stateful in a sense that subsequent calls to `__iter__` will continue previously created iterator.
    The returned iterator is essentially shared.

    This is especially useful when dealing with infinite or very large datasets. In those cases, we have to limit the epoch size during training.
    However, that introduces a problem : Most training cycle implementation will reset the dataloader iterator by calling `iter(dataloader)`, which makes
    us train on the same initial data over and over again. A `StatefulDataLoader` solves that issue by making calls to `__iter__` return the same shared iterator.

    Examples
    --------

    ```python
    from torch.utils.data import IterableDataset
    from silk.data.loader import StatefulDataLoader
    import itertools

    class MyDataset(IterableDataset):
        def __iter__(self):
            n = 1
            while True:
                yield n
                n += 1

    dataset = MyDataset()
    dataloader = StatefulDataLoader(dataset, batch_size=None, collate_fn=lambda x: x)

    print(list(itertools.islice(dataloader, 5)))
    # &gt;&gt;&gt; [1, 2, 3, 4, 5]
    print(list(itertools.islice(dataloader, 5)))
    # &gt;&gt;&gt; [6, 7, 8, 9, 10]
    ```
    &#34;&#34;&#34;

    class _Iterator(Iterator):
        &#34;&#34;&#34;Iternal iterator.

        IMPORTANT : This class HAS to subclass the abstract class `Iterator` since PyTorch Lightning runs the `next()` function on iterators by filtering them by class.
        In case this super class is removed, the iterator will not be processed properly.&#34;&#34;&#34;

        def __init__(self, data_loader, cycle) -&gt; None:
            super().__init__()
            self._cycle = cycle
            self._dataloader = data_loader
            self._reset_iterator()

        def _reset_iterator(self):
            self._iterator = torch.utils.data.DataLoader.__iter__(self._dataloader)

        def __next__(self):
            if self._cycle:
                try:
                    return next(self._iterator)
                except StopIteration:
                    self._reset_iterator()
            return next(self._iterator)

    def __init__(
        self,
        dataset: Union[Dataset, IterableDataset],
        *args: List[Any],
        cycle: bool = False,
        **kwargs: Dict[str, Any],
    ):
        &#34;&#34;&#34;

        Parameters
        ----------
        dataset : Union[Dataset, IterableDataset]
            Dataset provided to the dataloader (c.f. torch.utils.data.dataset.DataLoader).
        *args : List[Any]
            Positional arguments passed to the `DataLoader`.
        cycle : bool, optional
            Automatically reset the iterator once the end of the dataset is reached, by default False.
        **kwargs : Dict[str, Any]
            Keyword arguments passed to the `DataLoader`.
        &#34;&#34;&#34;
        super().__init__(dataset, *args, **kwargs)
        self._cycle = cycle
        self._stateful_iterator = None

    def reset(self):
        &#34;&#34;&#34;Resets the shared iterator to the beginning of the dataset.&#34;&#34;&#34;
        self._stateful_iterator = StatefulDataLoader._Iterator(self, self._cycle)

    def __iter__(self):
        if self._stateful_iterator is None:
            self.reset()
        return self._stateful_iterator</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.utils.data.dataloader.DataLoader</li>
<li>typing.Generic</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="silk.data.loader.StatefulDataLoader.batch_size"><code class="name">var <span class="ident">batch_size</span> : Optional[int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="silk.data.loader.StatefulDataLoader.dataset"><code class="name">var <span class="ident">dataset</span> : torch.utils.data.dataset.Dataset[+T_co]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="silk.data.loader.StatefulDataLoader.drop_last"><code class="name">var <span class="ident">drop_last</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="silk.data.loader.StatefulDataLoader.num_workers"><code class="name">var <span class="ident">num_workers</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="silk.data.loader.StatefulDataLoader.pin_memory"><code class="name">var <span class="ident">pin_memory</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="silk.data.loader.StatefulDataLoader.prefetch_factor"><code class="name">var <span class="ident">prefetch_factor</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="silk.data.loader.StatefulDataLoader.sampler"><code class="name">var <span class="ident">sampler</span> : Union[torch.utils.data.sampler.Sampler, Iterable[+T_co]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="silk.data.loader.StatefulDataLoader.timeout"><code class="name">var <span class="ident">timeout</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="silk.data.loader.StatefulDataLoader.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Resets the shared iterator to the beginning of the dataset.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self):
    &#34;&#34;&#34;Resets the shared iterator to the beginning of the dataset.&#34;&#34;&#34;
    self._stateful_iterator = StatefulDataLoader._Iterator(self, self._cycle)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="silk.data" href="index.html">silk.data</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="silk.data.loader.StatefulDataLoader" href="#silk.data.loader.StatefulDataLoader">StatefulDataLoader</a></code></h4>
<ul class="two-column">
<li><code><a title="silk.data.loader.StatefulDataLoader.batch_size" href="#silk.data.loader.StatefulDataLoader.batch_size">batch_size</a></code></li>
<li><code><a title="silk.data.loader.StatefulDataLoader.dataset" href="#silk.data.loader.StatefulDataLoader.dataset">dataset</a></code></li>
<li><code><a title="silk.data.loader.StatefulDataLoader.drop_last" href="#silk.data.loader.StatefulDataLoader.drop_last">drop_last</a></code></li>
<li><code><a title="silk.data.loader.StatefulDataLoader.num_workers" href="#silk.data.loader.StatefulDataLoader.num_workers">num_workers</a></code></li>
<li><code><a title="silk.data.loader.StatefulDataLoader.pin_memory" href="#silk.data.loader.StatefulDataLoader.pin_memory">pin_memory</a></code></li>
<li><code><a title="silk.data.loader.StatefulDataLoader.prefetch_factor" href="#silk.data.loader.StatefulDataLoader.prefetch_factor">prefetch_factor</a></code></li>
<li><code><a title="silk.data.loader.StatefulDataLoader.reset" href="#silk.data.loader.StatefulDataLoader.reset">reset</a></code></li>
<li><code><a title="silk.data.loader.StatefulDataLoader.sampler" href="#silk.data.loader.StatefulDataLoader.sampler">sampler</a></code></li>
<li><code><a title="silk.data.loader.StatefulDataLoader.timeout" href="#silk.data.loader.StatefulDataLoader.timeout">timeout</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>