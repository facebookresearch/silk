#!/bin/env python

"""SiLK Features.

Usage:
  silk-features [-o] [-m=<model>] [-c=<checkpoint>] [-s=<max_size>] [-k=<topk>] [-t=<threshold>] [-f=<format>] <images>...
  silk-features (-h | --help)
  silk-features --version

Options:
  -o --overwrite                Overwrite feature if already exist.
  -m --model=<model>            Model selection [default: pvgg-4].
  -c --checkpoint=<checkpoint>  Model checkpoint file.
  -s --max_size=<max_size>      Maximum image size allowed.
  -k --topk=<topk>              Top-k keypoints [default: 10_000].
  -t --threshold=<threshold>    Keypoint threshold [default: 1.].
  -f --format=<format>          Output format [default: torch].
  -h --help                     Show this screen.
  --version                     Show version.
"""

from docopt import docopt
from schema import Schema, And, Or, Use, SchemaError
from loguru import logger
import torch
import os
from torchvision.transforms.functional import resize, InterpolationMode

from silk.backbones.silk.silk import SiLKVGG
from silk.backbones.superpoint.vgg import ParametricVGG
from silk.config.model import load_model_from_checkpoint
from silk.backbones.silk.silk import from_feature_coords_to_image_coords
from silk.utils.cli import is_in, load_image

VALID_MODELS = {"pvgg-micro", "pvgg-1", "pvgg-2", "pvgg-3", "pvgg-4"}
VALID_FORMATS = {"torch"}
DEVICE = "cuda"
DEFAULT_CHECKPOINTS = {
    "pvgg-4": "assets/models/silk/analysis/alpha/pvgg-4.ckpt",
    "pvgg-3": "assets/models/silk/analysis/backbone/pvgg-3.ckpt",
    "pvgg-2": "assets/models/silk/analysis/backbone/pvgg-2.ckpt",
    "pvgg-1": "assets/models/silk/analysis/alpha/pvgg-1.ckpt",
    "pvgg-micro": "assets/models/silk/analysis/alpha/pvgg-micro.ckpt",
}


def load_model(model, checkpoint, topk, threshold):
    # start with defaults
    # ref : etc/backbones/silk-vgg.yaml
    klass = SiLKVGG
    kwargs = {
        "in_channels": 1,  # grayscale
        "detection_threshold": threshold,
        "detection_top_k": topk,
        "nms_dist": 0,
        "padding": 0,
        "border_dist": 0,
        "descriptor_scale_factor": 1.41,  # sqrt(2)
        "default_outputs": ("sparse_positions", "sparse_descriptors"),
    }

    if model.startswith("pvgg-"):
        # default VGG backbone
        # ref : etc/backbones/silk-pvgg-defaults.yaml
        backbone_klass = ParametricVGG
        backbone_kargs = {
            "input_num_channels": 1,
            "use_max_pooling": False,
            "padding": 0,
        }
        if model == "pvgg-micro":
            # ref : etc/backbones/silk-pvgg-micro.yaml
            kwargs["lat_channels"] = 32
            kwargs["desc_channels"] = 32
            kwargs["feat_channels"] = 64

            backbone_kargs["channels"] = (64,)
            backbone_kargs["normalization_fn"] = [torch.nn.BatchNorm2d(64)]
        elif model == "pvgg-1":
            # ref : etc/backbones/silk-pvgg-1.yaml
            backbone_kargs["channels"] = (128,)
            backbone_kargs["normalization_fn"] = [torch.nn.BatchNorm2d(128)]
        elif model == "pvgg-2":
            # ref : etc/backbones/silk-pvgg-2.yaml
            backbone_kargs["channels"] = (128, 128)
            backbone_kargs["normalization_fn"] = [
                torch.nn.BatchNorm2d(128),
                torch.nn.BatchNorm2d(128),
            ]
        elif model == "pvgg-3":
            # ref : etc/backbones/silk-pvgg-3.yaml
            backbone_kargs["channels"] = (64, 128, 128)
            backbone_kargs["normalization_fn"] = [
                torch.nn.BatchNorm2d(64),
                torch.nn.BatchNorm2d(128),
                torch.nn.BatchNorm2d(128),
            ]
        elif model == "pvgg-4":
            # ref : etc/backbones/silk-pvgg-4.yaml
            backbone_kargs["channels"] = (64, 64, 128, 128)
            backbone_kargs["normalization_fn"] = [
                torch.nn.BatchNorm2d(64),
                torch.nn.BatchNorm2d(64),
                torch.nn.BatchNorm2d(128),
                torch.nn.BatchNorm2d(128),
            ]

        kwargs["backbone"] = backbone_klass(**backbone_kargs)

    model = klass(**kwargs)

    model = load_model_from_checkpoint(
        model,
        checkpoint_path=checkpoint,
        state_dict_fn=lambda x: {k[len("_mods.model.") :]: v for k, v in x.items()},
        device=DEVICE,
        freeze=True,
        eval=True,
    )

    return model


def validate_args(args):
    schema = Schema(
        {
            "--overwrite": bool,
            "--model": And(
                is_in(VALID_MODELS),
                error=f"invalid model provided, should be one of {VALID_MODELS}",
            ),
            "--checkpoint": Or(None, str),
            "--max_size": Or(None, Use(int)),
            "--topk": Or("none", Use(int)),
            "--threshold": And(Use(float), lambda x: 0.0 <= x <= 1.0),
            "--format": And(
                is_in(VALID_FORMATS),
                error=f"invalid format provided, should be one of {VALID_FORMATS}",
            ),
            "--version": bool,
            "--help": bool,
            "<images>": list,
        }
    )
    try:
        args = schema.validate(args)
    except SchemaError as e:
        exit(e)
    return args


def set_default_checkpoints(args):
    if args["--checkpoint"] is None:
        args["--checkpoint"] = DEFAULT_CHECKPOINTS[args["--model"]]
    return args


def get_output_path(input_path, format):
    if format == "torch":
        path = f"{input_path}.pt"
    return path


def maybe_resize(image, max_size):
    # determine scale
    if max_size is not None:
        scale = max(max(image.shape[-2:]) / max_size, 1.0)
    else:
        scale = 1.0

    # resize if necessary
    if scale != 1.0:
        old_size = (image.shape[-2], image.shape[-1])
        new_size = (int(image.shape[-2] / scale), int(image.shape[-1] / scale))

        logger.warning(f"downsizing image from {old_size} to {new_size}")
        image = resize(
            image,
            size=new_size,
            interpolation=InterpolationMode.BILINEAR,
            antialias=None,
        )

    return image, scale


def write_output(output_path, image_path, keypoints, descriptors, format):
    # https://torch.github.io/tutorial.html#feature-detection-and-extraction
    if format == "torch":
        data = {
            "type": "keypoints",
            "image": image_path,
            "positions": keypoints[:, [1, 0]],  # x,y
            "score": keypoints[:, 2],
            "descriptors": descriptors,
        }
        torch.save(data, output_path)


if __name__ == "__main__":
    # get & check arguments
    args = docopt(__doc__, version="SiLK Features v0.1.0")
    args = validate_args(args)
    args = set_default_checkpoints(args)

    # load model
    logger.info(f"loading model {args['--model']}[{args['--checkpoint']}]")
    model = load_model(
        args["--model"], args["--checkpoint"], args["--topk"], args["--threshold"]
    )

    # compute features on list of images
    logger.info(f"computing SiLK feature for {len(args['<images>'])} images")
    for i, path in enumerate(args["<images>"]):
        logger.info(f"{i+1}/{len(args['<images>'])}")

        # check if keypoint output already exist
        output_path = get_output_path(path, args["--format"])
        if os.path.exists(output_path) and not args["--overwrite"]:
            logger.warning(
                f'output "{output_path}" already exists, skipping ... (enable the "--overwrite" flag to re-compuse features)'
            )
            continue

        # load image
        logger.info(f"loading : {path}")
        image = load_image(path, device=DEVICE)
        image, scale = maybe_resize(image, args["--max_size"])

        # extract keypoints
        logger.info(f"run model")
        keypoints, descriptors = model(image)
        keypoints, descriptors = keypoints[0], descriptors[0]
        keypoints = from_feature_coords_to_image_coords(model, keypoints)
        keypoints[:, :2] *= scale

        # dump keypoints to disk
        logger.info(f"dump keypoints to disk : {output_path}")
        write_output(output_path, path, keypoints, descriptors, args["--format"])

